= Manage VantageCloud Lake Compute Clusters with Apache Airflow
:experimental:
:page-author: Satish Chinthanippu
:page-email: satish.chinthanippu@teradata.com
:page-revdate: July 11th, 2024
:description: Manage VantageCloud Lake compute clusters with Apache Airflow
:keywords: data warehouses, compute storage separation, teradata, vantage, cloud data platform, business intelligence, enterprise analytics, airflow, workflow, teradatasql, ipython-sql, cloud computing, machine learning, vantagecloud, vantagecloud lake, lake
:dir: vantagecloud-lake-compute-cluster-airflow
:auxdir: vantagecloud-lake-compute-cluster-airflow

== Overview

This tutorial demonstrates the process of utilizing the https://airflow.apache.org/docs/apache-airflow-providers-teradata/stable/operators/index.html[Teradata Airflow Compute Cluster Operators] to manage VantageCloud Lake compute clusters. The objective is to execute dbt transformations defined on https://github.com/Teradata/jaffle_shop-dev.git[jaffle_shop] dbt project through VantageCloud Lake compute clusters.

NOTE: Use `https://learn.microsoft.com/en-us/windows/wsl/install[The Windows Subsystem for Linux (WSL)]` on `Windows` to try this quickstart example.

== Prerequisites
* Ensure you have the necessary credentials and access rights to use Teradata VantageCloud Lake.
+
include::vantagecloud-lake:partial$vantagecloud-lake-request.adoc[]
* Python 3.8, 3.9, 3.10 or 3.11 and python3-env, python3-pip installed.
+
[tabs, id="python_install"]
====
Linux::
+
[source,bash]
----
sudo apt install -y python3-venv python3-pip
----
WSL::
+
[source,bash]
----
sudo apt install -y python3-venv python3-pip
----
macOS::
+
[source,bash]
----
brew install python
----
 Refer https://docs.python-guide.org/starting/install3/osx/[Installation Guide] if you face any issues.
====

== Install Apache Airflow and Astronomer Cosmos
1. Create a new python environment to manage airflow and its dependencies. Activate the environment:
+
NOTE: This will install Apache Airflow as well.
+
[source, bash]
----
python3 -m venv airflow_env
source airflow_env/bin/activate
pip install "astronomer-cosmos"
----

+
2. Install the Apache Airflow Teradata provider
+
[source, bash]
----
pip install "apache-airflow-providers-teradata"
----
3. Set the AIRFLOW_HOME environment variable.
+
[source, bash]
----
export AIRFLOW_HOME=~/airflow
----

== Install dbt
1. Create a new python environment to manage dbt and its dependencies. Activate the environment:
+
[source, bash]
----
python3 -m venv dbt_env
source dbt_env/bin/activate
----
2. Install `dbt-teradata` and `dbt-core` modules:
+
[source, bash]
----
pip install dbt-teradata dbt-core
----

== Create a database

NOTE: A database client connected to VantageCloud Lake is needed to execute SQL statements. https://downloads.teradata.com/download/tools/vantage-editor-desktop[Vantage Editor Desktop], or https://quickstarts.teradata.com/other-integrations/configure-a-teradata-vantage-connection-in-dbeaver.html[dbeaver] can be used for this purpose.

Let's create the `jaffle_shop` database in the VantageCloud Lake instance with TD_OFSSTORAGE as default.

[source, teradata-sql]
----
CREATE DATABASE jaffle_shop
AS DEFAULT STORAGE = TD_OFSSTORAGE OVERRIDE ON ERROR,
PERMANENT = 120e6, -- 120MB
    SPOOL = 120e6; -- 120MB
----

== Create a database user

NOTE: A database client connected to VantageCloud Lake is needed to execute SQL statements. https://downloads.teradata.com/download/tools/vantage-editor-desktop[Vantage Editor Desktop], or https://quickstarts.teradata.com/other-integrations/configure-a-teradata-vantage-connection-in-dbeaver.html[dbeaver] can be used to execute `CREATE USER` query.

Let's create a `lake_user` user in the VantageCloud Lake instance.

[source, teradata-sql]
----
CREATE USER lake_user
AS PERMANENT = 1000000,
PASSWORD = lake_user,
SPOOL = 1200000,
DEFAULT DATABASE = jaffle_shop;
----

== Grant access to user

NOTE: A database client connected to VantageCloud Lake is needed to execute SQL statements. https://downloads.teradata.com/download/tools/vantage-editor-desktop[Vantage Editor Desktop], or https://quickstarts.teradata.com/other-integrations/configure-a-teradata-vantage-connection-in-dbeaver.html[dbeaver] can be used to execute `GRANT ACCESS` queries.

Let's provide the required privileges to the user `lake_user` to manage compute clusters.

[source, teradata-sql]
----
GRANT CREATE COMPUTE GROUP To lake_user;
GRANT DROP COMPUTE GROUP TO lake_user;
GRANT SELECT ON DBC TO lake_user;
GRANT ALL ON jaffle_shop TO lake_user;
----

== Setup dbt project

1. Clone the jaffle_shop repository and cd into the project directory:
+
[source, bash]
----
git clone https://github.com/Teradata/jaffle_shop-dev.git jaffle_shop
----
2. Make a new folder, dbt, inside $AIRFLOW_HOME/dags folder. Then, copy/paste jaffle_shop dbt project into $AIRFLOW_HOME/dags/dbt directory
+
[source, bash]
----
mkdir -p $AIRFLOW_HOME/dags/dbt/
cp -r jaffle_shop $AIRFLOW_HOME/dags/dbt/
----

== Configure Apache Airflow
1. Switch to virtual environment where Apache Airflow was installed at <<Install Apache Airflow and Astronomer Cosmos>>
+
[source, bash]
----
source airflow_env/bin/activate
----
2. Configure the listed environment variables to activate the test connection button, preventing the loading of sample DAGs and default connections in Airflow UI.
+
[source, bash]
  export AIRFLOW__CORE__TEST_CONNECTION=Enabled
  export AIRFLOW__CORE__LOAD_EXAMPLES=false
  export AIRFLOW__CORE_LOAD_DEFAULT_CONNECTIONS=false

3. Define the path of jaffle_shop project as an environment variable `dbt_project_home_dir`.
+
[source, bash]
----
export dbt_project_home_dir=$AIRFLOW_HOME/dags/dbt/jaffle_shop
----
4. Define the path to the virtual environment where dbt-teradata was installed as an environment variable `dbt_venv_dir`.
[source, bash]
export dbt_venv_dir=/../../dbt_env/bin/dbt
+
NOTE: You might need to change `/../../` to the specific path where the `dbt_env` virtual environment is located.

== Start Apache Airflow web server
1. Run airflow web server
+
[source, bash]
----
airflow standalone
----
2. Access the airflow UI. Visit https://localhost:8080 in the browser and log in with the admin account details shown in the terminal.
+
image::{dir}/execute-dbt-teradata-cosmos-airflow.png[Airflow Password,align="left" width=75%]

== Define a connection to VantageCloud Lake in Apache Airflow

1. Click on Admin - Connections
2. Click on + to define new connection to Teradata VantageCloud Lake instance.
3. Define new connection with id `teradata_lake` with Teradata VantageCloud Lake instance details.
* Connection Id: teradata_lake
* Connection Type: Teradata
* Database Server URL (required): Teradata VantageCloud Lake instance hostname  or IP to connect to.
* Database: jaffle_shop
* Login (required): lake_user
* Password (required): lake_user

== Define DAG in Apache Airflow
Dags in airflow are defined as python files. The dag below runs the dbt transformations defined in the `jaffle_shop` dbt project using VantageCloud Lake compute clusters. Copy the python code below and save it as `airflow-vcl-compute-clusters-manage.py` under the directory $AIRFLOW_HOME/files/dags.

[source, python]
----
from airflow.operators.bash import BashOperator
from datetime import datetime
from airflow import DAG
from airflow.providers.teradata.operators.teradata_compute_cluster import (
        TeradataComputeClusterDecommissionOperator,
        TeradataComputeClusterProvisionOperator,
        TeradataComputeClusterResumeOperator,
        TeradataComputeClusterSuspendOperator,
    )
from airflow.providers.teradata.operators.teradata import TeradataOperator

import os

PATH_TO_DBT_VENV = f"{os.environ['dbt_venv_dir']}"
PATH_TO_DBT_PROJECT = f"{os.environ['dbt_project_home_dir']}"
dbt_run_cmd = "dbt run "
with DAG(
    dag_id="explore_airflow_compute_cluster_operator",
    max_active_runs=1,
    max_active_tasks=10,
    catchup=False,
    start_date=datetime(2024, 1, 1),
    default_args={"teradata_conn_id": "teradata_lake"},

) as dag:
    # provision compute cluster with given configuration and initially its in suspended mode
    compute_cluster_provision_operation = TeradataComputeClusterProvisionOperator(
        task_id="compute_cluster_provision_operation",
        compute_profile_name="dbt_transformation_profile",
        compute_group_name="dbt_transformation_group",
        timeout=20,
        query_strategy="STANDARD",
        compute_map="TD_COMPUTE_XSMALL",
        compute_attribute="MIN_COMPUTE_COUNT(1) MAX_COMPUTE_COUNT(5) INITIALLY_SUSPENDED('TRUE')",
    )
    # assign dbt_transformation_group as default group to lake_user
    assign_compute_group_user = TeradataOperator(
        task_id="assign_compute_group_user",
        sql=r"""
        MODIFY USER lake_user
        AS COMPUTE GROUP = dbt_transformation_group;
        """,
    )
    # resume compute cluster
    compute_cluster_resume_operation = TeradataComputeClusterResumeOperator(
        task_id="compute_cluster_resume_operation",
        compute_profile_name="dbt_transformation_profile",
        compute_group_name="dbt_transformation_group",
    )

    # run dbt transformation defined in dbt jaffle_shop project
    transform_data = BashOperator(
        task_id="transform_data",
        bash_command="source $PATH_TO_DBT_VENV/bin/activate && dbt seed --profiles-dir $PATH_TO_DBT_PROJECT --project-dir $PATH_TO_DBT_PROJECT &&  dbt run --profiles-dir $PATH_TO_DBT_PROJECT --project-dir $PATH_TO_DBT_PROJECT -s path:models",
        env={"PATH_TO_DBT_PROJECT": PATH_TO_DBT_PROJECT, "PATH_TO_DBT_VENV": PATH_TO_DBT_VENV},
    )
    # suspend compute cluster
    compute_cluster_suspend_operation = TeradataComputeClusterSuspendOperator(
        task_id="compute_cluster_suspend_operation",
        compute_profile_name="dbt_transformation_profile",
        compute_group_name="dbt_transformation_group",
    )
    # decommission compute cluster
    compute_cluster_decommission_operation = TeradataComputeClusterDecommissionOperator(
        task_id="compute_cluster_decommission_operation",
        compute_profile_name="dbt_transformation_profile",
        compute_group_name="dbt_transformation_group",
        delete_compute_group=True,
    )
    # remove compute group for user
    remove_compute_group_from_user = TeradataOperator(
        task_id="remove_compute_group_from_user",
        sql=r"""
        MODIFY USER lake_user
        AS COMPUTE GROUP = NULL
        """,
    )

    (
        compute_cluster_provision_operation
        >> assign_compute_group_user
        >> compute_cluster_resume_operation
        >> transform_data
        >> compute_cluster_suspend_operation
        >> compute_cluster_decommission_operation
        >> remove_compute_group_from_user
    )
----
== Load DAG

When the dag file is copied to $AIRFLOW_HOME/dags, Apache Airflow displays the dag in UI under DAGs section. It will take 2 to 3 minutes to load DAG in Apache Airflow UI.

== Run DAG

Run the dag as shown in the image below.

image::{dir}/airflow-dag-run.png[Run dag,align="left" width=75%]

== Summary

In this quick start guide, we explored how to utilize Teradata VantageCloud Lake compute clusters to execute dbt transformations using Teradata compute cluster operators for Airflow.

== Further reading
* link:https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html[Apache Airflow DAGs reference]
* link:https://docs.teradata.com/r/Teradata-VantageCloud-Lake/Managing-Compute-Resources/Compute-Clusters[Teradata VantageCloud Lake Compute Clusters]
* link:https://airflow.apache.org/docs/apache-airflow-providers-teradata/stable/operators/index.html[Airflow Teradata Compute Cluster Operators]


