= Data Transfer from Amazon S3 to Teradata Vantage Using Apache Airflow
:experimental:
:page-author: Satish Chinthanippu
:page-email: satish.chinthanippu@teradata.com
:page-revdate: July 24th, 2024
:description: Transferring CSV, JSON, and Parquet data from Amazon S3 to Teradata Vantage with Airflow's S3 Cloud Transfer Operator
:keywords: data warehouses, teradata, vantage, transfer, cloud data platform, object storage, business intelligence, enterprise analytics, airflow, airflow teradata provider, amazon s3, s3
:dir: airflow-s3-to-teradata-transfer-operator-doc
:auxdir: airflow-s3-to-teradata-transfer-operator-doc

== Overview

This document provides instructions and guidance for transferring data in CSV, JSON and Parquet formats from Amazon S3 to Teradata Vantage using the Airflow Teradata Provider https://airflow.apache.org/docs/apache-airflow-providers-teradata/stable/operators/s3_to_teradata.html[S3 Cloud Transfer Operator]. It outlines the setup, configuration and execution steps required to establish a seamless data transfer pipeline between these platforms.

NOTE: Use `https://learn.microsoft.com/en-us/windows/wsl/install[The Windows Subsystem for Linux (WSL)]` on `Windows` to try this quickstart example.

== Prerequisites
* Access to a Teradata Vantage instance, version 17.10 or higher.
+
include::ROOT:partial$vantage_clearscape_analytics.adoc[]
* Python 3.8, 3.9, 3.10 or 3.11 and python3-env, python3-pip installed.
+
[tabs, id="python_install"]
====
Linux::
+
[source,bash]
----
sudo apt install -y python3-venv python3-pip
----
WSL::
+
[source,bash]
----
sudo apt install -y python3-venv python3-pip
----
macOS::
+
[source,bash]
----
brew install python
----
 Refer https://docs.python-guide.org/starting/install3/osx/[Installation Guide] if you face any issues.
====

== Install Apache Airflow
1. Create a new python environment to manage airflow and its dependencies. Activate the environment.
+
[source, bash]
----
python3 -m venv airflow_env
source airflow_env/bin/activate
AIRFLOW_VERSION=2.9.3
PYTHON_VERSION="$(python3 --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
----

+
2. Install the Apache Airflow Teradata provider package and the Amazon provider package.
+
[source, bash]
----
pip install "apache-airflow-providers-teradata[amazon]"
----
3. Set the AIRFLOW_HOME environment variable.
+
[source, bash]
----
export AIRFLOW_HOME=~/airflow
----

== Configure Apache Airflow
1. Switch to the virtual environment where Apache Airflow was installed at <<Install Apache Airflow>>
+
[source, bash]
----
source airflow_env/bin/activate
----
2. Configure the listed environment variables to activate the test connection button, preventing the loading of sample DAGs and default connections in the Airflow UI.
+
[source, bash]
  export AIRFLOW__CORE__TEST_CONNECTION=Enabled
  export AIRFLOW__CORE__LOAD_EXAMPLES=false
  export AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=false

== Start the Apache Airflow web server
1. Run airflow's web server
+
[source, bash]
----
airflow standalone
----
2. Access the airflow UI. Visit https://localhost:8080 in the browser and log in with the admin account details shown in the terminal.
+
image::{dir}/airflow-console-password.png[Airflow Password,align="left" width=75%]

== Define the Apache Airflow connection to Vantage

1. Click on Admin - Connections
2. Click on + to define a new connection to a Teradata Vantage instance.
3. Assign the new connection the id `teradata_default` with Teradata Vantage instance details.
* Connection Id: teradata_default
* Connection Type: Teradata
* Database Server URL (required): Teradata Vantage instance hostname to connect to.
* Database: database name
* Login (required): database user
* Password (required): database user password

Refer https://airflow.apache.org/docs/apache-airflow-providers-teradata/stable/connections/teradata.html[Teradata Connection] for more details.

== Define DAG in Apache Airflow
DAGs in airflow are defined as python files. The DAG below transfers data from Teradata-supplied public buckets to a Teradata Vantage instance. Copy the python code below and save it as `airflow-aws-to-teradata-transfer-operator-demo.py` under the directory $AIRFLOW_HOME/DAGs.

This DAG is a very simple example that covers:
- Droping the destination table if it exists
- Transfer of the data stored in object storage
- Get the number of transferred records
- Write the number of transferred records to the log

Refer https://airflow.apache.org/docs/apache-airflow-providers-teradata/stable/operators/s3_to_teradata.html[S3ToTeradataOperator] for more information on `Amazon S3 to Teradata` Transfer Operator.

[source, python]
----

from __future__ import annotations

import datetime

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.providers.teradata.operators.teradata import TeradataOperator
from airflow.providers.teradata.transfers.s3_to_teradata import S3ToTeradataOperator

DAG_ID = "example_aws_s3_to_teradata_transfer_operator"
CONN_ID = "teradata_default"
with DAG(
    dag_id=DAG_ID,
    start_date=datetime.datetime(2020, 2, 2),
    schedule="@once",
    catchup=False,
    default_args={"teradata_conn_id": CONN_ID},
) as dag:
    # Drop table if it exists
    drop_table_if_exists = TeradataOperator(
        task_id="drop_table_if_exists",
        sql="DROP table example_s3_teradata_csv;",
    )
    # Transfer data from S3 to Teradata Vantage instance
    transfer_data_csv = S3ToTeradataOperator(
        task_id="transfer_data_s3_to_teradata_csv",
        s3_source_key="/s3/td-usgs-public.s3.amazonaws.com/CSVDATA/09394500/2018/06/",
        public_bucket=True,
        teradata_table="example_s3_teradata_csv",
        teradata_conn_id="teradata_default",
        trigger_rule="always",
    )
    # Get the number of records transferred from S3 to teradata table
    read_data_table_csv = TeradataOperator(
        task_id="read_data_table_csv",
        sql="SELECT count(1) from example_s3_teradata_csv;",
    )
    # Print number of records in table
    print_number_of_records = BashOperator(
    task_id='print_number_of_records',
    bash_command="echo {{ ti.xcom_pull(task_ids='read_data_table_csv') }}",
    )
    (
       drop_table_if_exists
        >> transfer_data_csv
        >> read_data_table_csv
        >> print_number_of_records
    )
----

== Load DAG

When the DAG file is copied to $AIRFLOW_HOME/dags, Apache Airflow displays the DAG in the UI under the DAGs section. It will take 2 to 3 minutes to load the DAG in the Apache Airflow UI.

== Run DAG

Run the DAG as shown in the image below.

image::{dir}/DAG.png[Run DAG,align="left" width=75%]

== Transfer data from Private Amazon S3 bucket to Teradata instance
To successfully transfer data from a Private Amazon S3 bucket to a Teradata instance, the following prerequisites are necessary.

* Access to an https://aws.amazon.com[Amazon AWS account]
* Create a https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html[S3 bucket]
* https://docs.aws.amazon.com/AmazonS3/latest/userguide/uploading-an-object-bucket.html[Upload] CSV/JSON/Parquest format files to Private S3 bucket.
* https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey[Access Keys] to access AWS account
* Create a Teradata Authorization object with the AWS Account Key and the Account Secret Key
+
[source, teradata-sql]
----
CREATE AUTHORIZATION aws_authorization USER 'AWSAccessKey' PASSWORD 'AWSSecretAccessKey'
----
NOTE: Replace `AWSAccessKey` and `AWSSecretAccessKey` with your AWS account https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html[access key]
* Modify `blob_source_key` with YOUR-PRIVATE-OBJECT-STORE-URI in `transfer_data_csv` task and add `teradata_authorization_name` field with Teradata Authorization Object name
+
[source, python]
----
transfer_data_csv = S3ToTeradataOperator(
        task_id="transfer_data_blob_to_teradata_csv",
        s3_source_key="YOUR-OBJECT-STORE-URI",
        teradata_table="example_blob_teradata_csv",
        teradata_conn_id="teradata_default",
        teradata_authorization_name="aws_authorization",
        trigger_rule="always",
    )
----


== Summary
This guide details the utilization of the Airflow Teradata Providerâ€™s S3 Cloud Transfer Operator to seamlessly transfer CSV, JSON, and Parquet data from Amazon S3 to Teradata Vantage, facilitating streamlined data operations between these platforms.

== Further reading
* link:https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/DAGs.html[Apache Airflow DAGs reference]
* link:https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/SQL-Data-Definition-Language-Syntax-and-Examples/Authorization-Statements-for-External-Routines/CREATE-AUTHORIZATION-and-REPLACE-AUTHORIZATION[Teradata Authorization]
* link:https://learn.microsoft.com/en-us/windows/wsl/install[Install WSL on windows]

